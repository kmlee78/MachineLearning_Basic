{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(11610, 8)"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from tensorflow import keras\n",
    "from sklearn.datasets import fetch_california_housing\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "\n",
    "# 데이터 가져오고 전처리\n",
    "housing = fetch_california_housing()\n",
    "X_train_full, X_test, y_train_full, y_test = train_test_split(housing.data, housing.target)\n",
    "X_train, X_valid, y_train, y_valid = train_test_split(X_train_full, y_train_full)\n",
    "\n",
    "scaler = StandardScaler()\n",
    "# 갑작스럽지만 fit_transform은 평균 표준편차를 구하고 그거에 따라 정규화 하는거고 transform은 앞에서 구한 평균 표준편차에 따라 정규화만 함\n",
    "X_train = scaler.fit_transform(X_train)\n",
    "X_valid = scaler.transform(X_valid)\n",
    "X_test = scaler.transform(X_test)\n",
    "X_train.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train on 11610 samples, validate on 3870 samples\n",
      "Epoch 1/10\n",
      "11610/11610 [==============================] - 1s 63us/sample - loss: 0.7402 - val_loss: 12.8364\n",
      "Epoch 2/10\n",
      "11610/11610 [==============================] - 0s 36us/sample - loss: 1.1668 - val_loss: 0.4557\n",
      "Epoch 3/10\n",
      "11610/11610 [==============================] - 1s 44us/sample - loss: 0.5449 - val_loss: 0.4033\n",
      "Epoch 4/10\n",
      "11610/11610 [==============================] - 0s 34us/sample - loss: 0.4233 - val_loss: 0.3977\n",
      "Epoch 5/10\n",
      "11610/11610 [==============================] - 0s 34us/sample - loss: 0.4051 - val_loss: 0.3964\n",
      "Epoch 6/10\n",
      "11610/11610 [==============================] - 0s 36us/sample - loss: 0.4129 - val_loss: 0.3840\n",
      "Epoch 7/10\n",
      "11610/11610 [==============================] - 0s 34us/sample - loss: 0.4001 - val_loss: 0.3881\n",
      "Epoch 8/10\n",
      "11610/11610 [==============================] - 0s 36us/sample - loss: 0.4065 - val_loss: 0.3920\n",
      "Epoch 9/10\n",
      "11610/11610 [==============================] - 0s 38us/sample - loss: 0.3881 - val_loss: 0.3819\n",
      "Epoch 10/10\n",
      "11610/11610 [==============================] - 0s 37us/sample - loss: 0.3822 - val_loss: 0.3788\n",
      "5160/5160 [==============================] - 0s 19us/sample - loss: 0.3696\n",
      "0.3696388869784599\n",
      "[0.663 3.647 1.893]\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "array([[1.2573755],\n",
       "       [2.2307024],\n",
       "       [2.1249328]], dtype=float32)"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# 신경망 구축 및 모델 학습\n",
    "model = keras.models.Sequential([\n",
    "    keras.layers.Dense(30, activation='relu', input_shape=X_train.shape[1:]),\n",
    "    keras.layers.Dense(1)\n",
    "])\n",
    "\n",
    "model.compile(loss='mean_squared_error', optimizer='sgd')\n",
    "history = model.fit(X_train, y_train, epochs=10, validation_data=(X_valid, y_valid))\n",
    "\n",
    "mse_test = model.evaluate(X_test, y_test)\n",
    "X_new = X_test[:3]  # new샘플\n",
    "print(mse_test)\n",
    "print(y_test[:3])\n",
    "model.predict(X_new)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 복잡한 모델: 일부 특성은 더 짧은 경로로 가게\n",
    "\n",
    "input_A = keras.layers.Input(shape=[5], name='wide_input')\n",
    "input_B = keras.layers.Input(shape=[6], name='deep_input')\n",
    "hidden1 = keras.layers.Dense(30, activation='relu')(input_B) # input_B에서 hidden1층으로 연결한다는 소리\n",
    "hidden2 = keras.layers.Dense(30, activation='relu')(hidden1)  \n",
    "concat = keras.layers.concatenate([input_A, hidden2])  # input_A랑 hidden2층이랑 연결\n",
    "output = keras.layers.Dense(1)(concat)  # 결국 다 이 출력층으로 연결될 수 있게\n",
    "model = keras.Model(inputs=[input_A, input_B], outputs=[output])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train on 11610 samples, validate on 3870 samples\n",
      "Epoch 1/10\n",
      "11610/11610 [==============================] - 1s 64us/sample - loss: 2.3173 - val_loss: 1.1871\n",
      "Epoch 2/10\n",
      "11610/11610 [==============================] - 0s 43us/sample - loss: 0.9230 - val_loss: 0.8392\n",
      "Epoch 3/10\n",
      "11610/11610 [==============================] - 0s 42us/sample - loss: 0.7534 - val_loss: 0.7394\n",
      "Epoch 4/10\n",
      "11610/11610 [==============================] - 0s 38us/sample - loss: 0.6838 - val_loss: 0.6886\n",
      "Epoch 5/10\n",
      "11610/11610 [==============================] - 0s 42us/sample - loss: 0.6430 - val_loss: 0.6477\n",
      "Epoch 6/10\n",
      "11610/11610 [==============================] - 0s 37us/sample - loss: 0.6138 - val_loss: 0.6251\n",
      "Epoch 7/10\n",
      "11610/11610 [==============================] - 0s 40us/sample - loss: 0.5906 - val_loss: 0.6025\n",
      "Epoch 8/10\n",
      "11610/11610 [==============================] - 0s 39us/sample - loss: 0.5714 - val_loss: 0.5822\n",
      "Epoch 9/10\n",
      "11610/11610 [==============================] - 0s 38us/sample - loss: 0.5544 - val_loss: 0.5700\n",
      "Epoch 10/10\n",
      "11610/11610 [==============================] - 0s 37us/sample - loss: 0.5417 - val_loss: 0.5571\n",
      "5160/5160 [==============================] - 0s 19us/sample - loss: 0.5163\n",
      "0.5163073606269304\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "array([[1.4693388],\n",
       "       [1.8375902],\n",
       "       [2.2054548]], dtype=float32)"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.compile(loss='mse', optimizer=keras.optimizers.SGD(lr=1e-3))  # optimizer은 경사하강법(학습률 설정)\n",
    "\n",
    "X_train_A, X_train_B = X_train[:, :5], X_train[:, 2:]\n",
    "X_valid_A, X_valid_B = X_valid[:, :5], X_valid[:, 2:]\n",
    "X_test_A, X_test_B = X_test[:, :5], X_test[:, 2:]\n",
    "X_new_A, X_new_B = X_test_A[:3], X_test_B[:3]\n",
    "\n",
    "history = model.fit((X_train_A, X_train_B), y_train, epochs=10, validation_data=((X_valid_A, X_valid_B), y_valid))\n",
    "mse_test = model.evaluate((X_test_A, X_test_B), y_test)\n",
    "print(mse_test)\n",
    "model.predict((X_new_A, X_new_B))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 위 코드를 서브클래싱 API로 만들어서 사용해보자\n",
    "\n",
    "class WideAndDeepModel(keras.Model):\n",
    "    def __init__(self, units=30, activation='relu', **kwargs):\n",
    "        super().__init__(**kwargs)\n",
    "        self.hidden1 = keras.layers.Dense(units, activation=activation) \n",
    "        self.hidden2 = keras.layers.Dense(units, activation=activation)\n",
    "        self.main_output = keras.layers.Dense(1)\n",
    "        \n",
    "    def call(self, inputs):\n",
    "        input_A, input_B = inputs\n",
    "        hidden1 = self.hidden1(input_B)\n",
    "        hidden2 = self.hidden2(hidden1)\n",
    "        concat = keras.layers.concatenate([input_A, hidden2])\n",
    "        main_output = self.main_output(concat)\n",
    "        return main_output\n",
    "    \n",
    "model = WideAndDeepModel()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train on 11610 samples, validate on 3870 samples\n",
      "Epoch 1/10\n",
      "11610/11610 [==============================] - 1s 72us/sample - loss: 2.7905 - val_loss: 1.1421\n",
      "Epoch 2/10\n",
      "11610/11610 [==============================] - 0s 41us/sample - loss: 0.8606 - val_loss: 0.7784\n",
      "Epoch 3/10\n",
      "11610/11610 [==============================] - 0s 40us/sample - loss: 0.6941 - val_loss: 0.6766\n",
      "Epoch 4/10\n",
      "11610/11610 [==============================] - 0s 36us/sample - loss: 0.6306 - val_loss: 0.6196\n",
      "Epoch 5/10\n",
      "11610/11610 [==============================] - 1s 48us/sample - loss: 0.5894 - val_loss: 0.5826\n",
      "Epoch 6/10\n",
      "11610/11610 [==============================] - 1s 49us/sample - loss: 0.5589 - val_loss: 0.5545\n",
      "Epoch 7/10\n",
      "11610/11610 [==============================] - 1s 48us/sample - loss: 0.5364 - val_loss: 0.5363\n",
      "Epoch 8/10\n",
      "11610/11610 [==============================] - 0s 39us/sample - loss: 0.5189 - val_loss: 0.5218\n",
      "Epoch 9/10\n",
      "11610/11610 [==============================] - 0s 35us/sample - loss: 0.5048 - val_loss: 0.5099\n",
      "Epoch 10/10\n",
      "11610/11610 [==============================] - 0s 36us/sample - loss: 0.4924 - val_loss: 0.5038\n",
      "Model: \"wide_and_deep_model\"\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "dense_5 (Dense)              multiple                  210       \n",
      "_________________________________________________________________\n",
      "dense_6 (Dense)              multiple                  930       \n",
      "_________________________________________________________________\n",
      "dense_7 (Dense)              multiple                  36        \n",
      "=================================================================\n",
      "Total params: 1,176\n",
      "Trainable params: 1,176\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "model.compile(loss='mse', optimizer=keras.optimizers.SGD(lr=1e-3))\n",
    "history = model.fit((X_train_A, X_train_B), y_train, epochs=10, validation_data=((X_valid_A, X_valid_B), y_valid))\n",
    "model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 576x360 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "# 시각화\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "pd.DataFrame(history.history).plot(figsize=(8, 5))  \n",
    "plt.grid(True) \n",
    "plt.xlabel('epochs')\n",
    "plt.ylabel('loss')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train on 7740 samples, validate on 3870 samples\n",
      "Epoch 1/5\n",
      "7740/7740 [==============================] - 1s 68us/sample - loss: 4.6074 - val_loss: 14.7425\n",
      "Epoch 2/5\n",
      "7740/7740 [==============================] - 0s 42us/sample - loss: 3.4161 - val_loss: 13.8596\n",
      "Epoch 3/5\n",
      "7740/7740 [==============================] - 0s 34us/sample - loss: 2.5713 - val_loss: 13.2600\n",
      "Epoch 4/5\n",
      "7740/7740 [==============================] - 0s 35us/sample - loss: 1.9708 - val_loss: 12.8594\n",
      "Epoch 5/5\n",
      "7740/7740 [==============================] - 0s 35us/sample - loss: 1.5439 - val_loss: 12.5985\n",
      "3870/3870 [==============================] - 0s 18us/sample - loss: 3.0394\n",
      "Train on 7740 samples, validate on 3870 samples\n",
      "Epoch 1/5\n",
      "7740/7740 [==============================] - 1s 66us/sample - loss: 5.1238 - val_loss: 6.9686\n",
      "Epoch 2/5\n",
      "7740/7740 [==============================] - 0s 41us/sample - loss: 3.7701 - val_loss: 4.7378\n",
      "Epoch 3/5\n",
      "7740/7740 [==============================] - 0s 41us/sample - loss: 2.8372 - val_loss: 3.3347\n",
      "Epoch 4/5\n",
      "7740/7740 [==============================] - 0s 42us/sample - loss: 2.1875 - val_loss: 2.4400\n",
      "Epoch 5/5\n",
      "7740/7740 [==============================] - 0s 35us/sample - loss: 1.7321 - val_loss: 1.8591\n",
      "3870/3870 [==============================] - 0s 19us/sample - loss: 1.5520\n",
      "Train on 7740 samples, validate on 3870 samples\n",
      "Epoch 1/5\n",
      "7740/7740 [==============================] - 1s 92us/sample - loss: 5.6448 - val_loss: 5.6200\n",
      "Epoch 2/5\n",
      "7740/7740 [==============================] - 0s 39us/sample - loss: 4.1849 - val_loss: 4.0937\n",
      "Epoch 3/5\n",
      "7740/7740 [==============================] - 0s 41us/sample - loss: 3.1684 - val_loss: 3.0572\n",
      "Epoch 4/5\n",
      "7740/7740 [==============================] - 0s 39us/sample - loss: 2.4575 - val_loss: 2.3578\n",
      "Epoch 5/5\n",
      "7740/7740 [==============================] - 0s 39us/sample - loss: 1.9584 - val_loss: 1.8752\n",
      "3870/3870 [==============================] - 0s 19us/sample - loss: 1.6715\n",
      "Train on 7740 samples, validate on 3870 samples\n",
      "Epoch 1/5\n",
      "7740/7740 [==============================] - 1s 90us/sample - loss: 5.7465 - val_loss: 4.9369\n",
      "Epoch 2/5\n",
      "7740/7740 [==============================] - 0s 58us/sample - loss: 2.8509 - val_loss: 5.4734\n",
      "Epoch 3/5\n",
      "7740/7740 [==============================] - 0s 49us/sample - loss: 1.7510 - val_loss: 6.8346\n",
      "Epoch 4/5\n",
      "7740/7740 [==============================] - 0s 50us/sample - loss: 1.3365 - val_loss: 7.5687\n",
      "Epoch 5/5\n",
      "7740/7740 [==============================] - 0s 46us/sample - loss: 1.1587 - val_loss: 7.7778\n",
      "3870/3870 [==============================] - 0s 13us/sample - loss: 1.8934\n",
      "Train on 7740 samples, validate on 3870 samples\n",
      "Epoch 1/5\n",
      "7740/7740 [==============================] - 1s 83us/sample - loss: 4.3524 - val_loss: 4.7771\n",
      "Epoch 2/5\n",
      "7740/7740 [==============================] - 0s 51us/sample - loss: 2.2044 - val_loss: 3.7128\n",
      "Epoch 3/5\n",
      "7740/7740 [==============================] - 0s 50us/sample - loss: 1.4003 - val_loss: 3.0767\n",
      "Epoch 4/5\n",
      "7740/7740 [==============================] - 0s 47us/sample - loss: 1.0477 - val_loss: 2.5579\n",
      "Epoch 5/5\n",
      "7740/7740 [==============================] - 0s 38us/sample - loss: 0.9012 - val_loss: 2.0661\n",
      "3870/3870 [==============================] - 0s 19us/sample - loss: 0.8207\n",
      "Train on 7740 samples, validate on 3870 samples\n",
      "Epoch 1/5\n",
      "7740/7740 [==============================] - 1s 86us/sample - loss: 4.5822 - val_loss: 2.9191\n",
      "Epoch 2/5\n",
      "7740/7740 [==============================] - 0s 44us/sample - loss: 2.0070 - val_loss: 2.2744\n",
      "Epoch 3/5\n",
      "7740/7740 [==============================] - 0s 39us/sample - loss: 1.2611 - val_loss: 1.7988\n",
      "Epoch 4/5\n",
      "7740/7740 [==============================] - 0s 38us/sample - loss: 1.0115 - val_loss: 1.1856\n",
      "Epoch 5/5\n",
      "7740/7740 [==============================] - 0s 42us/sample - loss: 0.8768 - val_loss: 0.9414\n",
      "3870/3870 [==============================] - 0s 19us/sample - loss: 0.7920\n",
      "Train on 7740 samples, validate on 3870 samples\n",
      "Epoch 1/5\n",
      "7740/7740 [==============================] - 1s 84us/sample - loss: 0.7874 - val_loss: 0.5715\n",
      "Epoch 2/5\n",
      "7740/7740 [==============================] - 0s 42us/sample - loss: 0.5041 - val_loss: 0.5238\n",
      "Epoch 3/5\n",
      "7740/7740 [==============================] - 0s 43us/sample - loss: 0.4442 - val_loss: 0.5415\n",
      "Epoch 4/5\n",
      "7740/7740 [==============================] - 0s 41us/sample - loss: 0.4236 - val_loss: 0.5917\n",
      "Epoch 5/5\n",
      "7740/7740 [==============================] - 0s 40us/sample - loss: 0.4092 - val_loss: 0.6342\n",
      "3870/3870 [==============================] - 0s 25us/sample - loss: 0.4326\n",
      "Train on 7740 samples, validate on 3870 samples\n",
      "Epoch 1/5\n",
      "7740/7740 [==============================] - 1s 77us/sample - loss: 0.9037 - val_loss: 1.2325\n",
      "Epoch 2/5\n",
      "7740/7740 [==============================] - 0s 45us/sample - loss: 0.5449 - val_loss: 0.4678\n",
      "Epoch 3/5\n",
      "7740/7740 [==============================] - 0s 39us/sample - loss: 0.4403 - val_loss: 0.4563\n",
      "Epoch 4/5\n",
      "7740/7740 [==============================] - 0s 39us/sample - loss: 0.4660 - val_loss: 3.2875\n",
      "Epoch 5/5\n",
      "7740/7740 [==============================] - 0s 39us/sample - loss: 0.5086 - val_loss: 0.4123\n",
      "3870/3870 [==============================] - 0s 18us/sample - loss: 0.4274\n",
      "Train on 7740 samples, validate on 3870 samples\n",
      "Epoch 1/5\n",
      "7740/7740 [==============================] - 1s 76us/sample - loss: 1.3325 - val_loss: 0.5283\n",
      "Epoch 2/5\n",
      "7740/7740 [==============================] - 0s 42us/sample - loss: 0.5079 - val_loss: 0.4621\n",
      "Epoch 3/5\n",
      "7740/7740 [==============================] - 0s 42us/sample - loss: 0.4483 - val_loss: 0.4247\n",
      "Epoch 4/5\n",
      "7740/7740 [==============================] - 0s 41us/sample - loss: 0.4214 - val_loss: 0.4119\n",
      "Epoch 5/5\n",
      "7740/7740 [==============================] - 0s 44us/sample - loss: 0.4108 - val_loss: 0.4203\n",
      "3870/3870 [==============================] - 0s 18us/sample - loss: 0.4018\n",
      "Train on 7740 samples, validate on 3870 samples\n",
      "Epoch 1/5\n",
      "7740/7740 [==============================] - 1s 74us/sample - loss: 2.1026 - val_loss: 2.0158\n",
      "Epoch 2/5\n",
      "7740/7740 [==============================] - 0s 40us/sample - loss: 0.6948 - val_loss: 1.7397\n",
      "Epoch 3/5\n",
      "7740/7740 [==============================] - 0s 37us/sample - loss: 0.5973 - val_loss: 1.6873\n",
      "Epoch 4/5\n",
      "7740/7740 [==============================] - 0s 39us/sample - loss: 0.5510 - val_loss: 1.6487\n",
      "Epoch 5/5\n",
      "7740/7740 [==============================] - 0s 38us/sample - loss: 0.5207 - val_loss: 1.5874\n",
      "3870/3870 [==============================] - 0s 20us/sample - loss: 0.6871\n",
      "Train on 7740 samples, validate on 3870 samples\n",
      "Epoch 1/5\n",
      "7740/7740 [==============================] - 1s 72us/sample - loss: 2.0286 - val_loss: 1.1352\n",
      "Epoch 2/5\n",
      "7740/7740 [==============================] - 0s 40us/sample - loss: 0.8152 - val_loss: 0.7833\n",
      "Epoch 3/5\n",
      "7740/7740 [==============================] - 0s 42us/sample - loss: 0.6566 - val_loss: 0.6147\n",
      "Epoch 4/5\n",
      "7740/7740 [==============================] - 0s 35us/sample - loss: 0.5934 - val_loss: 0.6696\n",
      "Epoch 5/5\n",
      "7740/7740 [==============================] - 0s 39us/sample - loss: 0.5495 - val_loss: 0.5438\n",
      "3870/3870 [==============================] - 0s 19us/sample - loss: 0.5668\n",
      "Train on 7740 samples, validate on 3870 samples\n",
      "Epoch 1/5\n",
      "7740/7740 [==============================] - 1s 93us/sample - loss: 1.9724 - val_loss: 4.2690\n",
      "Epoch 2/5\n",
      "7740/7740 [==============================] - 0s 37us/sample - loss: 1.2318 - val_loss: 4.3580\n",
      "Epoch 3/5\n",
      "7740/7740 [==============================] - 0s 39us/sample - loss: 1.1257 - val_loss: 3.6184\n",
      "Epoch 4/5\n",
      "7740/7740 [==============================] - 0s 41us/sample - loss: 0.7099 - val_loss: 0.5735\n",
      "Epoch 5/5\n",
      "7740/7740 [==============================] - 0s 38us/sample - loss: 0.5573 - val_loss: 0.5464\n",
      "3870/3870 [==============================] - 0s 26us/sample - loss: 0.4990\n",
      "Train on 7740 samples, validate on 3870 samples\n",
      "Epoch 1/5\n",
      "7740/7740 [==============================] - 1s 69us/sample - loss: 5.4859 - val_loss: 7.2352\n",
      "Epoch 2/5\n",
      "7740/7740 [==============================] - 0s 41us/sample - loss: 3.6280 - val_loss: 5.4665\n",
      "Epoch 3/5\n",
      "7740/7740 [==============================] - 0s 43us/sample - loss: 2.4981 - val_loss: 4.3357\n",
      "Epoch 4/5\n",
      "7740/7740 [==============================] - 0s 40us/sample - loss: 1.8037 - val_loss: 3.5851\n",
      "Epoch 5/5\n",
      "7740/7740 [==============================] - 0s 40us/sample - loss: 1.3732 - val_loss: 3.0682\n",
      "3870/3870 [==============================] - 0s 17us/sample - loss: 1.4625\n",
      "Train on 7740 samples, validate on 3870 samples\n",
      "Epoch 1/5\n",
      "7740/7740 [==============================] - 1s 73us/sample - loss: 7.2308 - val_loss: 9.0156\n",
      "Epoch 2/5\n",
      "7740/7740 [==============================] - 0s 41us/sample - loss: 4.4971 - val_loss: 4.8952\n",
      "Epoch 3/5\n",
      "7740/7740 [==============================] - 0s 40us/sample - loss: 2.9634 - val_loss: 2.8980\n",
      "Epoch 4/5\n",
      "7740/7740 [==============================] - 0s 37us/sample - loss: 2.0708 - val_loss: 1.9323\n",
      "Epoch 5/5\n",
      "7740/7740 [==============================] - 0s 39us/sample - loss: 1.5416 - val_loss: 1.4204\n",
      "3870/3870 [==============================] - 0s 16us/sample - loss: 1.3971\n",
      "Train on 7740 samples, validate on 3870 samples\n",
      "Epoch 1/5\n",
      "7740/7740 [==============================] - 1s 67us/sample - loss: 7.1568 - val_loss: 9.3866\n",
      "Epoch 2/5\n",
      "7740/7740 [==============================] - 0s 43us/sample - loss: 4.2897 - val_loss: 5.1727\n",
      "Epoch 3/5\n",
      "7740/7740 [==============================] - 0s 37us/sample - loss: 2.7426 - val_loss: 3.0623\n",
      "Epoch 4/5\n",
      "7740/7740 [==============================] - 0s 37us/sample - loss: 1.8795 - val_loss: 1.9768\n",
      "Epoch 5/5\n",
      "7740/7740 [==============================] - 0s 39us/sample - loss: 1.3830 - val_loss: 1.4108\n",
      "3870/3870 [==============================] - 0s 19us/sample - loss: 1.1343\n"
     ]
    },
    {
     "ename": "RuntimeError",
     "evalue": "Cannot clone object <tensorflow.python.keras.wrappers.scikit_learn.KerasRegressor object at 0x0000016AD6AF2D88>, as the constructor either does not set or modifies parameter learning_rate",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mRuntimeError\u001b[0m                              Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-10-ed29d83959d0>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m     23\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     24\u001b[0m \u001b[0mrnd_search_cv\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mRandomizedSearchCV\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mkeras_reg\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mparams_distribs\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mn_iter\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;36m5\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mcv\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;36m3\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 25\u001b[1;33m \u001b[0mrnd_search_cv\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mfit\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mX_train\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0my_train\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mepochs\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;36m5\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mvalidation_data\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mX_valid\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0my_valid\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[1;32mC:\\Users\\jaomi\\anaconda3\\envs\\tens_2\\lib\\site-packages\\sklearn\\utils\\validation.py\u001b[0m in \u001b[0;36minner_f\u001b[1;34m(*args, **kwargs)\u001b[0m\n\u001b[0;32m     61\u001b[0m             \u001b[0mextra_args\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mlen\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0margs\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;33m-\u001b[0m \u001b[0mlen\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mall_args\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     62\u001b[0m             \u001b[1;32mif\u001b[0m \u001b[0mextra_args\u001b[0m \u001b[1;33m<=\u001b[0m \u001b[1;36m0\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 63\u001b[1;33m                 \u001b[1;32mreturn\u001b[0m \u001b[0mf\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m*\u001b[0m\u001b[0margs\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     64\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     65\u001b[0m             \u001b[1;31m# extra_args > 0\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mC:\\Users\\jaomi\\anaconda3\\envs\\tens_2\\lib\\site-packages\\sklearn\\model_selection\\_search.py\u001b[0m in \u001b[0;36mfit\u001b[1;34m(self, X, y, groups, **fit_params)\u001b[0m\n\u001b[0;32m    875\u001b[0m             \u001b[1;31m# of the params are estimators as well.\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    876\u001b[0m             self.best_estimator_ = clone(clone(base_estimator).set_params(\n\u001b[1;32m--> 877\u001b[1;33m                 **self.best_params_))\n\u001b[0m\u001b[0;32m    878\u001b[0m             \u001b[0mrefit_start_time\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mtime\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mtime\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    879\u001b[0m             \u001b[1;32mif\u001b[0m \u001b[0my\u001b[0m \u001b[1;32mis\u001b[0m \u001b[1;32mnot\u001b[0m \u001b[1;32mNone\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mC:\\Users\\jaomi\\anaconda3\\envs\\tens_2\\lib\\site-packages\\sklearn\\utils\\validation.py\u001b[0m in \u001b[0;36minner_f\u001b[1;34m(*args, **kwargs)\u001b[0m\n\u001b[0;32m     61\u001b[0m             \u001b[0mextra_args\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mlen\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0margs\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;33m-\u001b[0m \u001b[0mlen\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mall_args\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     62\u001b[0m             \u001b[1;32mif\u001b[0m \u001b[0mextra_args\u001b[0m \u001b[1;33m<=\u001b[0m \u001b[1;36m0\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 63\u001b[1;33m                 \u001b[1;32mreturn\u001b[0m \u001b[0mf\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m*\u001b[0m\u001b[0margs\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     64\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     65\u001b[0m             \u001b[1;31m# extra_args > 0\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mC:\\Users\\jaomi\\anaconda3\\envs\\tens_2\\lib\\site-packages\\sklearn\\base.py\u001b[0m in \u001b[0;36mclone\u001b[1;34m(estimator, safe)\u001b[0m\n\u001b[0;32m     85\u001b[0m             raise RuntimeError('Cannot clone object %s, as the constructor '\n\u001b[0;32m     86\u001b[0m                                \u001b[1;34m'either does not set or modifies parameter %s'\u001b[0m \u001b[1;33m%\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 87\u001b[1;33m                                (estimator, name))\n\u001b[0m\u001b[0;32m     88\u001b[0m     \u001b[1;32mreturn\u001b[0m \u001b[0mnew_object\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     89\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mRuntimeError\u001b[0m: Cannot clone object <tensorflow.python.keras.wrappers.scikit_learn.KerasRegressor object at 0x0000016AD6AF2D88>, as the constructor either does not set or modifies parameter learning_rate"
     ]
    }
   ],
   "source": [
    "\"\"\"가장 좋은 하이퍼파라미터 조합 고르기\"\"\"\n",
    "\n",
    "from scipy.stats import reciprocal  # 역수\n",
    "from sklearn.model_selection import RandomizedSearchCV\n",
    "import numpy as np\n",
    "\n",
    "def build_model(n_hidden=1, n_neurons=10, learning_rate=3e-3, input_shape=[8]):\n",
    "    model = keras.models.Sequential()\n",
    "    model.add(keras.layers.InputLayer(input_shape=input_shape))\n",
    "    for layer in range(n_hidden):\n",
    "        model.add(keras.layers.Dense(n_neurons, activation='relu'))\n",
    "    model.add(keras.layers.Dense(1))\n",
    "    model.compile(loss='mse', optimizer=keras.optimizers.SGD(lr=learning_rate))\n",
    "    return model\n",
    "\n",
    "keras_reg = keras.wrappers.scikit_learn.KerasRegressor(build_model)\n",
    "\n",
    "params_distribs = {\n",
    "    'n_hidden': [0, 1, 2],\n",
    "    'n_neurons': np.arange(1, 10),\n",
    "    'learning_rate': reciprocal(3e-4, 3e-2)\n",
    "}\n",
    "\n",
    "rnd_search_cv = RandomizedSearchCV(keras_reg, params_distribs, n_iter=3, cv=3)\n",
    "rnd_search_cv.fit(X_train, y_train, epochs=5, validation_data=(X_valid, y_valid))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python [conda env:tens_2]",
   "language": "python",
   "name": "conda-env-tens_2-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
