{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "LogisticRegression 0.8\n",
      "RandomForestClassifier 0.95\n",
      "SVC 1.0\n",
      "VotingClassifier 1.0\n"
     ]
    }
   ],
   "source": [
    "\"\"\"투표기반 분류기: 여러 종류의 분류기의 예측값을 모아 가장 투표수가 많은 클래스 선택\"\"\"\n",
    "\n",
    "from sklearn.datasets import make_moons\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.ensemble import VotingClassifier\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.svm import SVC\n",
    "from sklearn.metrics import accuracy_score\n",
    "\n",
    "X, y = make_moons(n_samples=100, noise=0.15)\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2)\n",
    "\n",
    "log_clf = LogisticRegression()\n",
    "rnd_clf = RandomForestClassifier()\n",
    "svm_clf = SVC()\n",
    "\n",
    "voting_clf = VotingClassifier(\n",
    "    estimators=[('lr', log_clf), ('rf', rnd_clf), ('svc', svm_clf)],\n",
    "    voting='hard'\n",
    ")\n",
    "voting_clf.fit(X_train, y_train)\n",
    "\n",
    "# 정확도 평가\n",
    "for clf in (log_clf, rnd_clf, svm_clf, voting_clf):\n",
    "    clf.fit(X_train, y_train)\n",
    "    y_pred = clf.predict(X_test)\n",
    "    print(clf.__class__.__name__, accuracy_score(y_test, y_pred))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.9375\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "1.0"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\"\"\"배깅: 중복을 허용하여 샘플링 후 각자 분류기 학습. 페이스팅: 중복 허용X\"\"\"\n",
    "\n",
    "from sklearn.ensemble import BaggingClassifier\n",
    "from sklearn.tree import DecisionTreeClassifier\n",
    "\n",
    "# 결정트리 분류기 500개, 무작위로 선택된 80개의 샘플로 훈련(페이스팅 하려면 bootstrap=False, n_jobs는 사용할 CPU 코어 수)\n",
    "bag_clf = BaggingClassifier(\n",
    "    DecisionTreeClassifier(), n_estimators=500,\n",
    "    max_samples=80, bootstrap=True, n_jobs=-1, oob_score=True)\n",
    "bag_clf.fit(X_train, y_train)\n",
    "y_pred = bag_clf.predict(X_test)\n",
    "\n",
    "# 선택되지 않은 샘플들(oob) 평가하기\n",
    "print(bag_clf.oob_score_)\n",
    "accuracy_score(y_test, y_pred)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"랜덤 포레스트: 위처럼 배깅() 안에 결정트리()를 넣는 것 보다 바로 랜덤포레스트 분류기\"\"\"\n",
    "\n",
    "rnd_clf = RandomForestClassifier(n_estimators=500, max_leaf_nodes=16, n_jobs=-1)\n",
    "rnd_clf.fit(X_train, y_train)\n",
    "\n",
    "y_pred_rf = rnd_clf.predict(X_test)\n",
    "\n",
    "# 엑스트라 트리(더욱 무작위): ExtraTreesClassifer()을 사용한다. 쓰는 방법은 랜덤 포레스트 분류기랑 동일"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "sepal length (cm) 0.09563307863284339\n",
      "sepal width (cm) 0.026066744209729126\n",
      "petal length (cm) 0.42890426255979636\n",
      "petal width (cm) 0.449395914597631\n"
     ]
    }
   ],
   "source": [
    "# 평균적으로 불순도를 얼마나 감쇠키는지 확인하여 특성의 중요도를 측정할 수 있다. features_importances_ 변수에 저장.\n",
    "from sklearn.datasets import load_iris\n",
    "\n",
    "iris = load_iris()\n",
    "rnd_clf = RandomForestClassifier(n_estimators=500, n_jobs=-1)\n",
    "rnd_clf.fit(iris['data'], iris['target'])\n",
    "for name, score in zip(iris['feature_names'], rnd_clf.feature_importances_):\n",
    "    print(name, score)   # 꽃잎의 길이와 너비가 비교적 더 중요하다는 것을 알 수 있다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "AdaBoostClassifier(base_estimator=DecisionTreeClassifier(max_depth=1),\n",
       "                   learning_rate=0.5, n_estimators=200)"
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\"\"\"부스팅: 약한 학습기를 여러 개 연결해 강한 학습기를 만드는 방법\"\"\"\n",
    "\n",
    "# 에이다부스트\n",
    "from sklearn.ensemble import AdaBoostClassifier\n",
    "\n",
    "ada_clf = AdaBoostClassifier(\n",
    "    DecisionTreeClassifier(max_depth=1), n_estimators=200,\n",
    "    algorithm='SAMME.R', learning_rate=0.5)\n",
    "ada_clf.fit(X_train, y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[20:02:53] WARNING: src/objective/regression_obj.cu:152: reg:linear is now deprecated in favor of reg:squarederror.\n",
      "[0]\tvalidation_0-rmse:0.467813\n",
      "Will train until validation_0-rmse hasn't improved in 2 rounds.\n",
      "[1]\tvalidation_0-rmse:0.439258\n",
      "[2]\tvalidation_0-rmse:0.415468\n",
      "[3]\tvalidation_0-rmse:0.393071\n",
      "[4]\tvalidation_0-rmse:0.37218\n",
      "[5]\tvalidation_0-rmse:0.356533\n",
      "[6]\tvalidation_0-rmse:0.344739\n",
      "[7]\tvalidation_0-rmse:0.332846\n",
      "[8]\tvalidation_0-rmse:0.325414\n",
      "[9]\tvalidation_0-rmse:0.313601\n",
      "[10]\tvalidation_0-rmse:0.308293\n",
      "[11]\tvalidation_0-rmse:0.292932\n",
      "[12]\tvalidation_0-rmse:0.278276\n",
      "[13]\tvalidation_0-rmse:0.271588\n",
      "[14]\tvalidation_0-rmse:0.258124\n",
      "[15]\tvalidation_0-rmse:0.252585\n",
      "[16]\tvalidation_0-rmse:0.240523\n",
      "[17]\tvalidation_0-rmse:0.235978\n",
      "[18]\tvalidation_0-rmse:0.22513\n",
      "[19]\tvalidation_0-rmse:0.221609\n",
      "[20]\tvalidation_0-rmse:0.211832\n",
      "[21]\tvalidation_0-rmse:0.208873\n",
      "[22]\tvalidation_0-rmse:0.206581\n",
      "[23]\tvalidation_0-rmse:0.204601\n",
      "[24]\tvalidation_0-rmse:0.195461\n",
      "[25]\tvalidation_0-rmse:0.193917\n",
      "[26]\tvalidation_0-rmse:0.192559\n",
      "[27]\tvalidation_0-rmse:0.185159\n",
      "[28]\tvalidation_0-rmse:0.178562\n",
      "[29]\tvalidation_0-rmse:0.177682\n",
      "[30]\tvalidation_0-rmse:0.177129\n",
      "[31]\tvalidation_0-rmse:0.17182\n",
      "[32]\tvalidation_0-rmse:0.171371\n",
      "[33]\tvalidation_0-rmse:0.16672\n",
      "[34]\tvalidation_0-rmse:0.166525\n",
      "[35]\tvalidation_0-rmse:0.162412\n",
      "[36]\tvalidation_0-rmse:0.163108\n",
      "[37]\tvalidation_0-rmse:0.159445\n",
      "[38]\tvalidation_0-rmse:0.159566\n",
      "[39]\tvalidation_0-rmse:0.156375\n",
      "[40]\tvalidation_0-rmse:0.157246\n",
      "[41]\tvalidation_0-rmse:0.157145\n",
      "Stopping. Best iteration:\n",
      "[39]\tvalidation_0-rmse:0.156375\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# 그레이디언트 부스팅: 샘플의 가중치가 아닌 이전 예측이 낳은 잔여 오차에 새로운 예측기를 학습시킴\n",
    "# xgboost라는 라이브러리로 GBRT는 물론 자동 조기 종료와 같은 기능도 제공한다.\n",
    "\n",
    "import xgboost\n",
    "\n",
    "xgb_reg = xgboost.XGBRegressor()\n",
    "xgb_reg.fit(X_train, y_train,\n",
    "           eval_set=[(X_test, y_test)], early_stopping_rounds=2)\n",
    "y_pred = xgb_reg.predict(X_test)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python [conda env:tens_2]",
   "language": "python",
   "name": "conda-env-tens_2-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
