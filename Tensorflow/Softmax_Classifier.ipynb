{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "분류 문제에서 결괏값이 각 항목이 답이 될 확률로 표시하는 방법.\n",
    "전체 합은 1\n",
    "\"\"\"\n",
    "import tensorflow as tf\n",
    "import numpy as np\n",
    "\n",
    "# 4개의 변수가 주여지면 3가지 종류 중 하나로 분류\n",
    "x_data = [[1, 2, 1, 1],\n",
    "          [2, 1, 3, 2],\n",
    "          [3, 1, 3, 4],\n",
    "          [4, 1, 5, 5],\n",
    "          [1, 7, 5, 5],\n",
    "          [1, 2, 5, 6],\n",
    "          [1, 6, 6, 6],\n",
    "          [1, 7, 7, 7]]\n",
    "\n",
    "# y 데이터는 one-hot encoding을 처리한 상태다.\n",
    "y_data = [[0, 0, 1],\n",
    "          [0, 0, 1],\n",
    "          [0, 0, 1],\n",
    "          [0, 1, 0],\n",
    "          [0, 1, 0],\n",
    "          [0, 1, 0],\n",
    "          [1, 0, 0],\n",
    "          [1, 0, 0]]\n",
    "\n",
    "# 데이터를 numpy 형태로 변환\n",
    "x_data = np.asarray(x_data, dtype=np.float32)\n",
    "y_data = np.asarray(y_data, dtype=np.float32)\n",
    "\n",
    "nb_classes = 3\n",
    "\"\"\"\n",
    "만약 y 데이터가 one-hot encodig이 되어 있지 않고 0, 1, 2 이런식으로 데이터가 주어졌다면\n",
    "y_one_hot = tf.one_hot(list(y_data), nb_classes)\n",
    "y_one_hot = tf.reshape(y_one_hot, [-1, nb_classes])\n",
    "이 두 코드로 데이터 전처리를 해준다.\n",
    "\"\"\"\n",
    "\n",
    "# 임의의 W, b 설정\n",
    "W = tf.Variable(tf.random.normal((4, nb_classes)), name='weight')\n",
    "b = tf.Variable(tf.random.normal((nb_classes,)), name='bias')\n",
    "variables = [W, b]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Softmax 함수를 tf.nn.softmax()로 구현해줄 수 있다.\n",
    "def hypothesis(X):\n",
    "    return tf.nn.softmax(tf.matmul(X, W) + b)\n",
    "\n",
    "# 우리가 지금까지 쓰던 손실함수. 이를 cross entropy 라고 한다.\n",
    "def cost_fn(X, Y):\n",
    "    logits = hypothesis(X)\n",
    "    cost = -tf.reduce_sum(Y * tf.math.log(logits), axis=1)\n",
    "    cost_mean = tf.reduce_mean(cost)\n",
    "    return cost_mean\n",
    "\n",
    "def grad_fn(X, Y):\n",
    "    with tf.GradientTape() as tape:\n",
    "        loss = cost_fn(X, Y)\n",
    "        grads = tape.gradient(loss, variables)\n",
    "        return grads"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loss at epoch 1: 1.781103\n",
      "Loss at epoch 100: 0.813492\n",
      "Loss at epoch 200: 0.650811\n",
      "Loss at epoch 300: 0.576483\n",
      "Loss at epoch 400: 0.523067\n",
      "Loss at epoch 500: 0.478316\n",
      "Loss at epoch 600: 0.438194\n",
      "Loss at epoch 700: 0.400699\n",
      "Loss at epoch 800: 0.364465\n",
      "Loss at epoch 900: 0.328387\n",
      "Loss at epoch 1000: 0.291736\n",
      "Loss at epoch 1100: 0.256818\n",
      "Loss at epoch 1200: 0.238184\n",
      "Loss at epoch 1300: 0.226397\n",
      "Loss at epoch 1400: 0.215683\n",
      "Loss at epoch 1500: 0.205896\n",
      "Loss at epoch 1600: 0.196922\n",
      "Loss at epoch 1700: 0.188665\n",
      "Loss at epoch 1800: 0.181043\n",
      "Loss at epoch 1900: 0.173988\n",
      "Loss at epoch 2000: 0.167441\n"
     ]
    }
   ],
   "source": [
    "def fit(X, Y, epochs=2000, verbose=100):\n",
    "    optimizer =  tf.keras.optimizers.SGD(learning_rate=0.1)\n",
    "\n",
    "    for i in range(epochs):\n",
    "        grads = grad_fn(X, Y)\n",
    "        optimizer.apply_gradients(zip(grads, variables))\n",
    "        if (i==0) | ((i+1)%verbose==0):\n",
    "            print('Loss at epoch %d: %f' %(i+1, cost_fn(X, Y).numpy()))\n",
    "            \n",
    "fit(x_data, y_data)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python [conda env:tens_2]",
   "language": "python",
   "name": "conda-env-tens_2-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
